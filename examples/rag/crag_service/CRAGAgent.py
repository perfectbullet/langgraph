import os
from pathlib import Path
import uuid
from typing import Any, Dict, List, Optional

import requests
from dotenv import load_dotenv
from langchain_community.chat_models import ChatOllama, ChatOpenAI
from langchain_community.tools.tavily_search import TavilySearchResults
from langchain_community.vectorstores import Chroma
from langchain_core.documents import Document
from langchain_core.embeddings import Embeddings
from langchain_core.output_parsers import JsonOutputParser, StrOutputParser
from langchain_core.prompts import PromptTemplate
from langgraph.graph import END, START, StateGraph
from pydantic import BaseModel, Field
from typing_extensions import TypedDict
from typing import AsyncIterator, Iterator

# ==================== Embedding å®ç° ====================
class SiliconFlowEmbeddings(Embeddings):
    def __init__(self, model: str, api_key: str, base_url: str, batch_size: int = 32):
        self.model = model
        self.api_key = api_key
        self.base_url = base_url
        self.batch_size = batch_size

    def _embed_batch(self, texts: List[str]) -> List[List[float]]:
        payload = {"model": self.model, "input": texts}
        headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json"
        }
        response = requests.post(self.base_url, json=payload, headers=headers)
        print(response)
        result = response.json()
        if result.get("code") not in (None, 0):
            raise ValueError(f"Embedding request failed: {result}")
        data = result.get("data")
        if not data:
            raise ValueError(f"No embedding data returned: {result}")
        return [item["embedding"] for item in data]

    def embed_documents(self, texts: List[str]) -> List[List[float]]:
        embeddings: List[List[float]] = []
        for i in range(0, len(texts), self.batch_size):
            batch = texts[i:i + self.batch_size]
            embeddings.extend(self._embed_batch(batch))
        return embeddings

    def embed_query(self, text: str) -> List[float]:
        return self.embed_documents([text])[0]
    
class OpenAIStyleEmbeddings(Embeddings):
    """é€‚é… OpenAI /v1/embeddings é£æ ¼æ¥å£çš„åµŒå…¥å®ç°"""

    def __init__(
        self,
        model: str,
        base_url: str,
        api_key: Optional[str] = None,
        timeout: float = 30.0,
    ) -> None:
        self.model = model
        self.base_url = base_url.rstrip("/")
        self.api_key = api_key
        self.timeout = timeout

    def _embed_batch(self, texts: List[str]) -> List[List[float]]:
        payload = {"input": list(texts), "model": self.model}
        headers = {
            "Content-Type": "application/json",
            "User-Agent": "crag-service/1.0",
        }
        if self.api_key:
            headers["Authorization"] = f"Bearer {self.api_key}"
            print(f"Using API Key for Embedding service authentication.{self.api_key}")

        response = requests.post(
            f"{self.base_url}/v1/embeddings",
            json=payload,
            headers=headers,
            timeout=self.timeout,
        )
        response.raise_for_status()
        result = response.json()

        data = result.get("data")
        if not data:
            raise ValueError(f"Embedding service returned no data: {result}")
        return [item["embedding"] for item in data]

    def embed_documents(self, texts: List[str]) -> List[List[float]]:
        return self._embed_batch(texts)

    def embed_query(self, text: str) -> List[float]:
        return self._embed_batch([text])[0]


# ==================== Graph State ====================
class GraphState(TypedDict):
    """å›¾çŠ¶æ€å®šä¹‰"""

    question: str
    generation: str
    search: str
    documents: List[Document]
    steps: List[str]


# ==================== API Models ====================
class Message(BaseModel):
    """æ¶ˆæ¯æ¨¡å‹"""

    role: str = Field(..., description="è§’è‰²: user, assistant, system")
    content: str = Field(..., description="æ¶ˆæ¯å†…å®¹")


class ChatCompletionRequest(BaseModel):
    """èŠå¤©è¡¥å…¨è¯·æ±‚ (OpenAI å…¼å®¹)"""

    model: str = Field(default="crag-agent", description="æ¨¡å‹åç§°")
    messages: List[Message] = Field(..., description="å¯¹è¯æ¶ˆæ¯åˆ—è¡¨")
    temperature: float = Field(default=0.0, ge=0.0, le=2.0)
    max_tokens: Optional[int] = Field(default=None, ge=1)
    stream: bool = Field(default=False, description="æ˜¯å¦æµå¼è¿”å›")
    metadata: Optional[Dict[str, Any]] = Field(default=None)


class ChatCompletionResponse(BaseModel):
    """èŠå¤©è¡¥å…¨å“åº” (OpenAI å…¼å®¹)"""

    id: str = Field(..., description="å“åº”ID")
    object: str = Field(default="chat.completion", description="å¯¹è±¡ç±»å‹")
    created: int = Field(..., description="åˆ›å»ºæ—¶é—´æˆ³")
    model: str = Field(..., description="ä½¿ç”¨çš„æ¨¡å‹")
    choices: List[Dict[str, Any]] = Field(..., description="ç”Ÿæˆçš„å›å¤")
    usage: Dict[str, int] = Field(..., description="Token ä½¿ç”¨ç»Ÿè®¡")
    metadata: Optional[Dict[str, Any]] = Field(
        default=None, description="æ‰§è¡Œè½¨è¿¹ç­‰å…ƒæ•°æ®"
    )


class HealthResponse(BaseModel):
    """å¥åº·æ£€æŸ¥å“åº”"""

    status: str
    version: str
    model: str


# ==================== CRAG Agent å°è£… ====================
class CRAGAgent:
    """çº æ­£æ€§æ£€ç´¢å¢å¼ºç”Ÿæˆæ™ºèƒ½ä½“"""

    def __init__(
        self
    ):
        # åŠ è½½ç¯å¢ƒå˜é‡
        load_dotenv()

        # åˆå§‹åŒ– Embedding
        # self.embedding = SiliconFlowEmbeddings(
        #     model=os.getenv("EMBEDDING_MODEL", "BAAI/bge-large-zh-v1.5"),
        #     base_url=os.getenv("EMBEDDING_API_URL", "http://localhost:50009"),
        #     api_key=os.environ.get("SILICONFLOW_API_KEY", None),
        # )
        self.embedding = OpenAIStyleEmbeddings(
            model=os.getenv("EMBEDDING_MODEL", "BAAI/bge-large-zh-v1.5"),
            base_url=os.getenv("EMBEDDING_BASE_URL", "http://localhost:50009"),
        )

        # åŠ è½½å‘é‡æ•°æ®åº“
        chroma_db_dir = os.getenv("CHROMA_DB_DIR", "chroma_db_for_crag_local_zenking")
        self.vectorstore = self._load_vectorstore(chroma_db_dir)
        self.retriever = self.vectorstore.as_retriever(k=4)

        if os.environ['USE_OLLAMA']:
            # åˆå§‹åŒ– LLM
            self.llm = ChatOllama(
                base_url=os.environ['OLLAMA_BASE_URL'],
                model=os.environ['OLLAMA_MODEL'],
                temperature=0,
                streaming=True,
            )

            self.grader_llm = ChatOllama(
                base_url=os.environ['OLLAMA_BASE_URL'],
                model=os.environ.get('OLLAMA_GRADER_MODEL', 'qwen2.5:7b'),  # å°æ¨¡å‹
                temperature=0,  # 
                format="json",  # å¼ºåˆ¶ JSON
            )
        else:
            self.llm = ChatOpenAI(
                base_url = "https://api.siliconflow.cn/v1", 
                api_key=os.environ['SILICONFLOW_API_KEY'], 
                model="deepseek-ai/DeepSeek-V3.1-Terminus", 
                temperature=0, 
                streaming=True
            )
            self.grader_llm = ChatOpenAI(
                base_url="https://api.siliconflow.cn/v1",
                api_key=os.environ['SILICONFLOW_API_KEY'],
                model="deepseek-ai/DeepSeek-V3",  # éæ¨ç†ç‰ˆæœ¬ï¼Œå¿«é€Ÿè¯„åˆ†
                temperature=0,
                model_kwargs={"response_format": {"type": "json_object"}},
            )

        # åˆå§‹åŒ– Web æœç´¢å·¥å…·
        self.web_search_tool = TavilySearchResults(k=3)

        # æ„å»º LangGraph
        self.graph = self._build_graph()
        self._dump_graph_debug()
        # ä¿å­˜å›¾ç»“æ„ä»¥ä¾›è°ƒè¯•

    def _dump_graph_debug(self) -> None:
        """ä¿å­˜å›¾ç»“æ„ç”¨äºè°ƒè¯•"""
        dump_flag = os.getenv("CRAG_DUMP_GRAPH", "1").lower()
        if dump_flag in {"0", "false", "no"}:
            return

        try:
            graph_view = self.graph.get_graph(xray=True)
            png_bytes = graph_view.draw_mermaid_png()
            output_dir = Path(os.getenv("CRAG_GRAPH_DIR", "./graph_debug"))
            output_dir.mkdir(parents=True, exist_ok=True)
            target = output_dir / "crag_graph.png"
            target.write_bytes(png_bytes)
            print(f"âœ“ Graph debug saved at {target}")
        except Exception as exc:
            print(f"âš ï¸ Unable to dump graph visualization: {exc}")
            
    def _load_vectorstore(self, chroma_db_dir: str) -> Chroma:
        """åŠ è½½å‘é‡æ•°æ®åº“"""
        if not os.path.exists(chroma_db_dir) or not os.listdir(chroma_db_dir):
            raise ValueError(
                f"å‘é‡æ•°æ®åº“ç›®å½•ä¸å­˜åœ¨æˆ–ä¸ºç©º: {chroma_db_dir}\n"
                "è¯·å…ˆè¿è¡Œ è„šæœ¬ åˆ›å»ºå‘é‡æ•°æ®åº“"
            )

        print(f"âœ“ åŠ è½½å‘é‡æ•°æ®åº“: {chroma_db_dir}")
        return Chroma(
            collection_name="rag_local_markdown_docs",
            embedding_function=self.embedding,
            persist_directory=chroma_db_dir,
        )

    def _build_graph(self) -> StateGraph:
        """æ„å»º LangGraph å·¥ä½œæµ"""
        # åˆ›å»º Retrieval Grader
        retrieval_grader_prompt = PromptTemplate(
            template="""ä½ æ˜¯ä¸€åæ‰¹æ”¹å°æµ‹éªŒçš„æ•™å¸ˆã€‚ä½ å°†æ”¶åˆ°ä»¥ä¸‹ä¸¤é¡¹å†…å®¹ï¼š
1. ä¸€ä¸ªé—®é¢˜ï¼ˆQUESTIONï¼‰
2. å­¦ç”Ÿæä¾›çš„ä¸€ä¸ªäº‹å®ä¾æ®ï¼ˆFACTï¼‰

ä½ éœ€è¦å¯¹"ç›¸å…³æ€§å¬å›ç‡"ï¼ˆRELEVANCE RECALLï¼‰è¿›è¡Œè¯„åˆ†ï¼Œè¯„åˆ†è§„åˆ™å¦‚ä¸‹ï¼š
- è‹¥äº‹å®ä¾æ®ï¼ˆFACTï¼‰ä¸­çš„**ä»»æ„ä¸€æ¡è¡¨è¿°**ä¸é—®é¢˜ï¼ˆQUESTIONï¼‰ç›¸å…³ï¼Œè¯„åˆ†å³ä¸º1ã€‚
- è‹¥äº‹å®ä¾æ®ï¼ˆFACTï¼‰ä¸­çš„**æ‰€æœ‰è¡¨è¿°**å‡ä¸é—®é¢˜ï¼ˆQUESTIONï¼‰æ— å…³ï¼Œè¯„åˆ†å³ä¸º0ã€‚

è¯·ç»™å‡º"yes"æˆ–"no"çš„äºŒå…ƒè¯„åˆ†ï¼Œä»¥è¡¨æ˜è¯¥äº‹å®ä¾æ®ï¼ˆæ–‡æ¡£ï¼‰æ˜¯å¦ä¸é—®é¢˜ç›¸å…³ã€‚
è¯·å°†äºŒå…ƒè¯„åˆ†ä»¥JSONæ ¼å¼å‘ˆç°ï¼Œä»…åŒ…å«"score"è¿™ä¸€ä¸ªé”®ï¼Œä¸”æ— éœ€å‰ç¼€è¯´æ˜æˆ–é¢å¤–è§£é‡Šã€‚

é—®é¢˜ï¼š{question}
äº‹å®ä¾æ®ï¼š{documents}
""",
            input_variables=["question", "documents"],
        )
        self.retrieval_grader = retrieval_grader_prompt | self.llm | JsonOutputParser()

        # åˆ›å»º RAG Chain
        rag_prompt = PromptTemplate(
            template="""ä½ æ˜¯ä¸€ä¸ªé—®ç­”ä»»åŠ¡åŠ©æ‰‹ã€‚

ä½¿ç”¨ä»¥ä¸‹æ–‡æ¡£æ¥å›ç­”é—®é¢˜ã€‚
å¦‚æœä½ ä¸çŸ¥é“ç­”æ¡ˆï¼Œå°±è¯´ä½ ä¸çŸ¥é“ã€‚
ä½¿ç”¨æœ€å¤šä¸‰å¥è¯ï¼Œå¹¶ä¿æŒç­”æ¡ˆç®€æ´ï¼š
é—®é¢˜: {question}
æ–‡æ¡£: {documents}
ç­”æ¡ˆ:
""",
            input_variables=["question", "documents"],
        )
        self.rag_chain = rag_prompt | self.llm | StrOutputParser()

        # å®šä¹‰èŠ‚ç‚¹å‡½æ•°
        def retrieve(state):
            """æ£€ç´¢æ–‡æ¡£"""
            question = state["question"]
            documents = self.retriever.invoke(question)
            steps = state.get("steps", [])
            steps.append("retrieve_documents")
            return {"documents": documents, "question": question, "steps": steps}

        def grade_documents(state):
            """è¯„åˆ†æ–‡æ¡£å¹¶å†³å®šæ˜¯å¦éœ€è¦ web search"""
            question = state["question"]
            documents = state["documents"]
            steps = state.get("steps", [])
            steps.append("grade_document_retrieval")

            filtered_docs = []

            print("documents çš„ä¸ªæ•°:", len(documents))
            for d in documents:
                score = self.retrieval_grader.invoke(
                    {"question": question, "documents": d.page_content}
                )
                grade = score.get("score", "no")
                print(f"æ–‡æ¡£è¯„åˆ†: {grade}")

                if grade in ["1", "yes", "Yes", 1, True]:
                    filtered_docs.append(d)
            return {
                "documents": filtered_docs,
                "question": question,
                "steps": steps,
            }

        def web_search(state):
            """æ‰§è¡Œ web æœç´¢"""
            question = state["question"]
            documents = state.get("documents", [])
            steps = state.get("steps", [])
            steps.append("web_search")

            web_results = self.web_search_tool.invoke({"query": question})
            documents.extend(
                [
                    Document(page_content=d["content"], metadata={"url": d["url"]})
                    for d in web_results
                ]
            )
            return {"documents": documents, "question": question, "steps": steps}

        def generate(state):
            """ç”Ÿæˆæœ€ç»ˆç­”æ¡ˆ"""
            question = state["question"]
            documents = state["documents"]
            generation = self.rag_chain.invoke(
                {"documents": documents, "question": question}
            )
            steps = state.get("steps", [])
            steps.append("generate_answer")
            return {
                "documents": documents,
                "question": question,
                "generation": generation,
                "steps": steps,
            }

        # å†³ç­–å‡½æ•°åªè¿”å›ä¸‹ä¸€æ­¥çš„èŠ‚ç‚¹å
        def decide_to_generate(state):
            """å†³å®šæ˜¯ web search è¿˜æ˜¯ç›´æ¥ç”Ÿæˆç­”æ¡ˆ"""
            # filtered_docsä¸ºç©ºæ—¶ï¼Œè§¦å‘ web search
            documents = state["documents"]
            search = "Yes" if len(documents) == 0 else "No"
            print(f"å†³ç­–: documents æ•°é‡={len(documents)}, search={search}")
            if search == "Yes":
                return "search"  # éœ€è¦ web search
            else:
                return "generate"  # ç›´æ¥ç”Ÿæˆç­”æ¡ˆ

        # æ„å»ºå›¾æ—¶ç¡®ä¿æµç¨‹å•å‘
        workflow = StateGraph(GraphState)
        workflow.add_node("retrieve", retrieve)
        workflow.add_node("grade_documents", grade_documents)
        workflow.add_node("generate", generate)
        workflow.add_node("web_search", web_search)

        # è®¾ç½®è¾¹
        workflow.add_edge(START, "retrieve")
        workflow.add_edge("retrieve", "grade_documents")

        # å…³é”®:æ¡ä»¶è¾¹åªæœ‰ä¸¤ä¸ªå‡ºå£,ä¸ä¼šå¾ªç¯å› retrieve
        workflow.add_conditional_edges(
            "grade_documents",
            decide_to_generate,
            {
                "search": "web_search",  # ä¸ç›¸å…³ â†’ web search
                "generate": "generate",  # ç›¸å…³ â†’ ç”Ÿæˆç­”æ¡ˆ
            },
        )

        # Web search åç›´æ¥ç”Ÿæˆ,ä¸å†å›åˆ° retrieve
        workflow.add_edge("web_search", "generate")
        workflow.add_edge("generate", END)

        return workflow.compile()

    def invoke(self, question: str) -> Dict[str, Any]:
        """æ‰§è¡Œ CRAG æŸ¥è¯¢ï¼ˆéæµå¼ï¼‰"""
        config = {"configurable": {"thread_id": str(uuid.uuid4())}}
        state_dict = self.graph.invoke({"question": question, "steps": []}, config)
        return {
            "response": state_dict["generation"],
            "steps": state_dict["steps"],
            "documents": [
                {"content": d.page_content, "metadata": d.metadata}
                for d in state_dict.get("documents", [])
            ],
        }

    def stream(self, question: str) -> Iterator[Dict[str, Any]]:
        """
        æ‰§è¡Œ CRAG æŸ¥è¯¢ï¼ˆæµå¼è¾“å‡ºï¼‰
        
        Yields:
            dict: åŒ…å« type å’Œ content çš„æµå¼æ•°æ®
                - type: "step" | "chunk" | "metadata" | "done"
                - content: å¯¹åº”çš„æ•°æ®
        """
        config = {"configurable": {"thread_id": str(uuid.uuid4())}}
        
        # ç¬¬ä¸€æ­¥ï¼šæµå¼æ‰§è¡Œå›¾èŠ‚ç‚¹
        collected_steps = []
        collected_docs = []
        final_question = question
        
        for event in self.graph.stream(
            {"question": question, "steps": []}, 
            config,
            stream_mode="values"
        ):
            # å‘é€æ­¥éª¤ä¿¡æ¯
            if "steps" in event and event["steps"]:
                current_step = event["steps"][-1]
                if current_step not in collected_steps:
                    collected_steps.append(current_step)
                    yield {
                        "type": "step",
                        "content": current_step
                    }
            
            # æ”¶é›†æ–‡æ¡£ä¿¡æ¯
            if "documents" in event:
                collected_docs = event["documents"]
            
            # å¦‚æœåˆ°è¾¾ generate èŠ‚ç‚¹ï¼Œå¼€å§‹æµå¼è¾“å‡ºç”Ÿæˆå†…å®¹
            if "generation" in event and event.get("generation"):
                # æ³¨æ„ï¼šè¿™é‡Œ event["generation"] æ˜¯å®Œæ•´å­—ç¬¦ä¸²
                # å¦‚æœéœ€è¦çœŸæ­£çš„ token çº§æµå¼ï¼Œéœ€è¦ä¿®æ”¹ generate èŠ‚ç‚¹
                generation = event["generation"]
                
                # æ¨¡æ‹Ÿåˆ†å—å‘é€ï¼ˆå®é™…åº”åœ¨ generate èŠ‚ç‚¹ä¸­ä½¿ç”¨æµå¼ LLMï¼‰
                chunk_size = 10  # æ¯æ¬¡å‘é€ 10 ä¸ªå­—ç¬¦
                for i in range(0, len(generation), chunk_size):
                    chunk = generation[i:i + chunk_size]
                    yield {
                        "type": "chunk",
                        "content": chunk
                    }
        
        # å‘é€å…ƒæ•°æ®
        yield {
            "type": "metadata",
            "content": {
                "steps": collected_steps,
                "documents_count": len(collected_docs),
                "documents": [
                    {"content": d.page_content[:200], "metadata": d.metadata}
                    for d in collected_docs
                ]
            }
        }
        
        # å‘é€å®Œæˆæ ‡å¿—
        yield {
            "type": "done",
            "content": None
        }

    async def astream(self, question: str) -> AsyncIterator[Dict[str, Any]]:
        """
        å¼‚æ­¥æµå¼æ‰§è¡Œï¼ˆç”¨äº FastAPIï¼‰
        """
        config = {"configurable": {"thread_id": str(uuid.uuid4())}}
        
        collected_steps = []
        collected_docs = []
        
        async for event in self.graph.astream(
            {"question": question, "steps": []},
            config,
            stream_mode="values"
        ):
            if "steps" in event and event["steps"]:
                current_step = event["steps"][-1]
                if current_step not in collected_steps:
                    collected_steps.append(current_step)
                    yield {
                        "type": "step",
                        "content": current_step
                    }
            
            if "documents" in event:
                collected_docs = event["documents"]
            
            if "generation" in event and event.get("generation"):
                generation = event["generation"]
                chunk_size = 10
                for i in range(0, len(generation), chunk_size):
                    chunk = generation[i:i + chunk_size]
                    yield {
                        "type": "chunk",
                        "content": chunk
                    }
        
        yield {
            "type": "metadata",
            "content": {
                "steps": collected_steps,
                "documents_count": len(collected_docs),
            }
        }
        
        yield {"type": "done", "content": None}


# ==================== Standalone Testing ====================
if __name__ == "__main__":

    print("=== CRAG Agent Standalone Test ===\n")
    import os

    from dotenv import load_dotenv

    # åŠ è½½ .env æ–‡ä»¶ä¸­çš„ç¯å¢ƒå˜é‡
    load_dotenv()

    # éªŒè¯å¿…éœ€çš„ç¯å¢ƒå˜é‡æ˜¯å¦å·²è®¾ç½®
    required_env_vars = ["SILICONFLOW_API_KEY",  "TAVILY_API_KEY", "EMBEDDING_API_URL"]

    for var in required_env_vars:
        if not os.environ.get(var):
            raise ValueError(f"ç¯å¢ƒå˜é‡ {var} æœªè®¾ç½®ï¼Œè¯·åœ¨ .env æ–‡ä»¶ä¸­é…ç½®")
        print(f"âœ“ {var} å·²åŠ è½½")

    # é…ç½®å‚æ•°
    CHROMA_DB_DIR = os.getenv("CHROMA_DB_DIR", "./chroma_db")
    # åˆå§‹åŒ– Agent
    print("åˆå§‹åŒ– CRAG Agent...")
    agent = CRAGAgent()
    print("Agent åˆå§‹åŒ–æˆåŠŸ\n")

    # æµ‹è¯•æŸ¥è¯¢
    test_questions = [
        # "ä»€ä¹ˆæ˜¯ LangGraph?",
        # "å¦‚ä½•ä½¿ç”¨ CRAG æå‡æ£€ç´¢è´¨é‡?",
        "å¤±èœ¡é“¸é€ åŸç†",
        "åŒ—äº¬å¤©æ°”æ€ä¹ˆæ ·ï¼Ÿ",
    ]

    for i, question in enumerate(test_questions, 1):
        print(f"\n{'='*60}")
        print(f"æµ‹è¯•æŸ¥è¯¢ {i}: {question}")
        print("="*60)

        result = agent.invoke(question)

        print(f"\nğŸ“ å“åº”:\n{result['response']}")
        print(f"\nğŸ” æ‰§è¡Œæ­¥éª¤: {' â†’ '.join(result['steps'])}")
        print(f"\nğŸ“š ä½¿ç”¨æ–‡æ¡£æ•°é‡: {len(result['documents'])}")

        if result["documents"]:
            print("\næ–‡æ¡£æ‘˜è¦:")
            for idx, doc in enumerate(result["documents"][:2], 1):
                content_preview = doc["content"][:100].replace("\n", " ")
                print(f"  {idx}. {content_preview}...")

    print("\n\nâœ“ æµ‹è¯•å®Œæˆ")
    