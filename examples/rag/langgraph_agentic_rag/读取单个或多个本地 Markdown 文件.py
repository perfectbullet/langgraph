from langchain_community.document_loaders import TextLoader, DirectoryLoader, UnstructuredMarkdownLoader
from langchain_community.vectorstores import Chroma
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.embeddings import ZhipuAIEmbeddings
from pathlib import Path
import os

# âœ… æ–¹å¼1: è¯»å–å•ä¸ª Markdown æ–‡ä»¶
def load_single_markdown(file_path: str):
    """åŠ è½½å•ä¸ª Markdown æ–‡ä»¶"""
    loader = UnstructuredMarkdownLoader(file_path)
    docs = loader.load()
    return docs

# âœ… æ–¹å¼2: è¯»å–å¤šä¸ª Markdown æ–‡ä»¶
def load_multiple_markdowns(file_paths: list):
    """åŠ è½½å¤šä¸ª Markdown æ–‡ä»¶"""
    docs_list = []
    for file_path in file_paths:
        try:
            loader = UnstructuredMarkdownLoader(file_path)
            docs = loader.load()
            docs_list.extend(docs)
            print(f"âœ… æˆåŠŸåŠ è½½: {file_path}")
        except Exception as e:
            print(f"âŒ åŠ è½½å¤±è´¥ {file_path}: {e}")
    return docs_list

# âœ… æ–¹å¼3: è¯»å–æ•´ä¸ªç›®å½•ä¸‹çš„æ‰€æœ‰ Markdown æ–‡ä»¶
def load_markdown_directory(directory_path: str):
    """åŠ è½½ç›®å½•ä¸‹æ‰€æœ‰ Markdown æ–‡ä»¶"""
    loader = DirectoryLoader(
        directory_path,
        glob="**/*.md",  # åŒ¹é…æ‰€æœ‰ .md æ–‡ä»¶
        loader_cls=UnstructuredMarkdownLoader,
        show_progress=True
    )
    docs = loader.load()
    return docs


# === ä½¿ç”¨ç¤ºä¾‹ ===

# ç¤ºä¾‹1: åŠ è½½å•ä¸ªæ–‡ä»¶
print("=" * 60)
print("ğŸ“„ ç¤ºä¾‹1: åŠ è½½å•ä¸ª Markdown æ–‡ä»¶")
print("=" * 60)

single_file = "d:/ai_works/documents/example.md"  # ä¿®æ”¹ä¸ºæ‚¨çš„æ–‡ä»¶è·¯å¾„
if os.path.exists(single_file):
    docs_list = load_single_markdown(single_file)
    print(f"åŠ è½½äº† {len(docs_list)} ä¸ªæ–‡æ¡£")
else:
    print(f"âš ï¸ æ–‡ä»¶ä¸å­˜åœ¨: {single_file}")


# ç¤ºä¾‹2: åŠ è½½å¤šä¸ªæŒ‡å®šçš„æ–‡ä»¶
print("\n" + "=" * 60)
print("ğŸ“„ ç¤ºä¾‹2: åŠ è½½å¤šä¸ª Markdown æ–‡ä»¶")
print("=" * 60)

md_files = [
    "d:/ai_works/documents/file1.md",
    "d:/ai_works/documents/file2.md",
    "d:/ai_works/documents/file3.md",
]

# è¿‡æ»¤å­˜åœ¨çš„æ–‡ä»¶
existing_files = [f for f in md_files if os.path.exists(f)]
if existing_files:
    docs_list = load_multiple_markdowns(existing_files)
    print(f"âœ… æ€»å…±åŠ è½½äº† {len(docs_list)} ä¸ªæ–‡æ¡£")
else:
    print("âš ï¸ æ²¡æœ‰æ‰¾åˆ°ä»»ä½•æ–‡ä»¶")


# ç¤ºä¾‹3: åŠ è½½æ•´ä¸ªç›®å½•
print("\n" + "=" * 60)
print("ğŸ“ ç¤ºä¾‹3: åŠ è½½ç›®å½•ä¸‹æ‰€æœ‰ Markdown æ–‡ä»¶")
print("=" * 60)

docs_directory = "d:/ai_works/documents"  # ä¿®æ”¹ä¸ºæ‚¨çš„ç›®å½•è·¯å¾„
if os.path.exists(docs_directory):
    docs_list = load_markdown_directory(docs_directory)
    print(f"âœ… ä»ç›®å½•åŠ è½½äº† {len(docs_list)} ä¸ªæ–‡æ¡£")
else:
    print(f"âš ï¸ ç›®å½•ä¸å­˜åœ¨: {docs_directory}")


# === æ–‡æ¡£åˆ‡åˆ†å’Œå‘é‡åŒ– ===

print("\n" + "=" * 60)
print("âœ‚ï¸ æ–‡æ¡£åˆ‡åˆ†")
print("=" * 60)

# ä½¿ç”¨ RecursiveCharacterTextSplitter åˆ‡åˆ†æ–‡æ¡£
text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(
    chunk_size=100, 
    chunk_overlap=50
)

doc_splits = text_splitter.split_documents(docs_list)
print(f'âœ… åˆ‡åˆ†åçš„æ–‡æ¡£å—æ•°é‡: {len(doc_splits)}')

# æ‰“å°å‰å‡ ä¸ªåˆ‡åˆ†çš„ç¤ºä¾‹
for i, split in enumerate(doc_splits[:3], 1):
    print(f"\n--- æ–‡æ¡£å— {i} ---")
    print(f"å†…å®¹: {split.page_content[:200]}...")
    print(f"å…ƒæ•°æ®: {split.metadata}")


# === åˆ›å»ºå‘é‡å­˜å‚¨ ===

print("\n" + "=" * 60)
print("ğŸ” åˆ›å»ºå‘é‡å­˜å‚¨")
print("=" * 60)

# ä½¿ç”¨æ™ºè°± AI Embeddings
embeddings = ZhipuAIEmbeddings(
    model="embedding-2",  # æˆ–ä½¿ç”¨å…¶ä»–æ¨¡å‹
    api_key="your_zhipu_api_key"  # æ›¿æ¢ä¸ºæ‚¨çš„ API Key
)

# åˆ›å»º Chroma å‘é‡æ•°æ®åº“
vectorstore = Chroma.from_documents(
    documents=doc_splits,
    embedding=embeddings,
    collection_name="local_markdown_docs",
    persist_directory="./chroma_db"  # æŒä¹…åŒ–å­˜å‚¨è·¯å¾„
)

print("âœ… å‘é‡å­˜å‚¨åˆ›å»ºæˆåŠŸï¼")

# æµ‹è¯•æ£€ç´¢
query = "ä»€ä¹ˆæ˜¯ LangChain?"
results = vectorstore.similarity_search(query, k=3)
print(f"\næŸ¥è¯¢: {query}")
print(f"æ‰¾åˆ° {len(results)} ä¸ªç›¸å…³ç»“æœ")